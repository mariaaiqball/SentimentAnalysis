{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2123fc",
   "metadata": {},
   "source": [
    "#### AI-Powered Brand Sentiment Tracking from Twitter Trends: Naive Bayes \n",
    "\n",
    "We wanted to explore how AI could track brand sentiment across competitors.In this notebook, we will go through how we achieved implementing Naive Bayes. At the end, we will compare all CSV files for both starbucks and dunkin to understand our results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91686fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT STATEMENTS\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import math\n",
    "import naive_bayes as nb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1c280",
   "metadata": {},
   "source": [
    "#### 1. PREPARE AND LOAD DATA \n",
    "\n",
    "Cleaning Data. What did that entail?\n",
    "- Pulling data from X. We used search_tweets.py to pull about 500 tweets. \n",
    "- We had to manually label our data. \n",
    "- We had to figure out what information was important for our model. \n",
    "For our 'starbucks.csv' data, that left us with 435 tweets. No duplicates, all tweets in english, no links, no mentions, no rewteets. \n",
    "To test our model, we will use starbucks.csv to start doing sentiment analysis.\n",
    "\n",
    "Using the pandas library, we read in our csv file and dropped duplicates and null values, reindexed our data, and then visualized our data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21951ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broccoli head just came in and had me make a l...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I took my dog with me to Starbucks &amp;amp; to go...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tomorrow Is Teacher Appreciation Week So All M...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why did starbucks just call us out infront of ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My cousins jst posted a story of them on a Sta...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1. give cool houseless guy a fiver as he said ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>My American girlfriend will not drink fresh sq...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>theyâ€™re bringing back the blue drink at starbu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Itâ€™s too many Starbucks stores and they keep c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Trump is my President!!!  His ability to infur...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text     label\n",
       "0    broccoli head just came in and had me make a l...   neutral\n",
       "1    I took my dog with me to Starbucks &amp; to go...   neutral\n",
       "2    Tomorrow Is Teacher Appreciation Week So All M...  positive\n",
       "3    why did starbucks just call us out infront of ...   neutral\n",
       "4    My cousins jst posted a story of them on a Sta...   neutral\n",
       "..                                                 ...       ...\n",
       "430  1. give cool houseless guy a fiver as he said ...   neutral\n",
       "431  My American girlfriend will not drink fresh sq...   neutral\n",
       "432  theyâ€™re bringing back the blue drink at starbu...  positive\n",
       "433  Itâ€™s too many Starbucks stores and they keep c...  negative\n",
       "434  Trump is my President!!!  His ability to infur...   neutral\n",
       "\n",
       "[435 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the starbucks data and save it into a dataframe. Remove any duplicates and reindex data. \n",
    "df = pd.read_csv('starbucks.csv')\n",
    "df.drop_duplicates(subset=['text'], keep='last', inplace=True)\n",
    "df.dropna(subset=['text','label'], inplace=True)\n",
    "df.dropna()\n",
    "df.reindex()\n",
    "df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccd8c6",
   "metadata": {},
   "source": [
    "#### 2. SPLIT OUR DATA \n",
    "\n",
    "We used the scikit libaray to split our data. Using their parameters, we split our data 30-70 and used the stratify param to ensure we had equal number of positive, neutral, and negative data training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e45da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d909f",
   "metadata": {},
   "source": [
    "#### 3. PREPROCESS DATA \n",
    "Our first step in implementing our data into the alogorithm is preprocessing our data. That means, taking every tweet, breaking up the sentence into indivdiuals words and emojis, and saving it into an array as one of our tokens. We apply the preprocessing function to all of our text data using the .apply() function. We can test out how our data will look under the 'Tokens' col by printing out the top 5 messages using .head(). We wanted to incorporate emojis because it is such an integral part of communication. Saying \"I love going grocery shopping â¤ï¸\" and \"I love going grocery shopping ðŸ™„\" show two different sentiments using the same exact sentence. \n",
    "We applied our function to all of the positive, negative, and neutral messages in our train_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19a7aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Message: i love going grocery shopping â¤ï¸\n",
      "Tokenized Message: ['love', 'going', 'grocery', 'shopping', 'â¤ï¸']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, remove_stopwords=True):\n",
    "    # lowercase all words \n",
    "    lowercase = text.lower()\n",
    "    # tokenize both words and emojis \n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\"\n",
    "                        \"\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\"\n",
    "                        \"\\U00002700-\\U000027BF\\U0001F900-\\U0001F9FF\"\n",
    "                        \"\\U00002600-\\U000026FF\\u200d]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9\\s\" + emoji_pattern.pattern + \"]\", \"\", lowercase)\n",
    "    tokens = text.split()\n",
    "    #remove stopwords \n",
    "    stop_words = stopwords.words('english')\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    return tokens \n",
    "\n",
    "test_message = 'i love going grocery shopping â¤ï¸'\n",
    "tokenized_message = preprocess_text(test_message)\n",
    "print(f'Test Message: {test_message}')\n",
    "print(f'Tokenized Message: {tokenized_message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a0190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the training data and split it into positive, negative, and neutral dataframes\n",
    "train_positive = train_df[train_df['label']=='positive']\n",
    "train_negative = train_df[train_df['label']=='negative']\n",
    "train_neutral = train_df[train_df['label']=='neutral']\n",
    "\n",
    "#tokenize \n",
    "pos_counts = Counter()\n",
    "neg_counts = Counter()\n",
    "neu_counts = Counter()\n",
    "\n",
    "for text in train_positive['text']:\n",
    "    toks = preprocess_text(text)\n",
    "    pos_counts.update(toks)\n",
    "for text in train_negative['text']:\n",
    "    toks = preprocess_text(text)\n",
    "    neg_counts.update(toks)\n",
    "for text in train_neutral['text']:\n",
    "    toks = preprocess_text(text)\n",
    "    neu_counts.update(toks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188121cb",
   "metadata": {},
   "source": [
    "#### 4. TERM FREQUENCY VECTORS\n",
    "\n",
    "We calculated the number of unique words that were in our training data to understand our vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f46b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2047\n",
      "Total positive tokens: 1003\n"
     ]
    }
   ],
   "source": [
    "positive_tokens_list = [list(pos_counts)] \n",
    "negative_tokens_list = [list(neg_counts)]\n",
    "neutral_tokens_list  = [list(neu_counts)]\n",
    "\n",
    "positive_word_counts = pos_counts\n",
    "negative_word_counts = neg_counts\n",
    "neutral_word_counts  = neu_counts\n",
    "\n",
    "# total tokens per class\n",
    "total_positive_tokens = sum(pos_counts.values())\n",
    "total_negative_tokens = sum(neg_counts.values())\n",
    "total_neutral_tokens  = sum(neu_counts.values())\n",
    "\n",
    "# build vocab as union\n",
    "vocab = set(pos_counts) | set(neg_counts) | set(neu_counts)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Total positive tokens: {total_positive_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54504a7a",
   "metadata": {},
   "source": [
    "#### 5. CONDITIONAL PROBABILITY SMOOTHING\n",
    "\n",
    "After that, we implemented the conditional probabilty function using laplace smoothing. We applied conditional probabilty to every token and returned whether each sentence was either positive, negative, or neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc778e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: I just went to this Paris Baguette bakery coffee place in Uptown Phoenix and I can see why Starbucks is having trouble bouncing back. So many other bakery coffee places are popping up in so many places and are way better than Starbucks.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "Message: Omg I just tried the new matcha latte at starbucks and it's SO good!\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Message: been at starbucks for 40 minutes and dont even have a page of this damn paper done this is bad\n",
      "Predicted Sentiment: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_positive = len(train_positive)\n",
    "num_negative = len(train_negative)\n",
    "num_neutral = len(train_neutral)\n",
    "num_total = len(train_df)\n",
    "\n",
    "def conditional_probability_smoothing(word, class_label):\n",
    "    V = len(vocab)\n",
    "    if class_label == 'positive':\n",
    "        count = positive_word_counts.get(word, 0)\n",
    "        return (count + 1) / (total_positive_tokens + V)\n",
    "    elif class_label == 'negative':\n",
    "        count = negative_word_counts.get(word, 0)\n",
    "        return (count + 1) / (total_negative_tokens + V)\n",
    "    elif class_label == 'neutral':\n",
    "        count = neutral_word_counts.get(word, 0)\n",
    "        return (count + 1) / (total_neutral_tokens + V)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid class\")\n",
    "\n",
    "# Compute the prior probabilities: P(positive), P(negative), P(neutral)\n",
    "P_positive = num_positive/num_total\n",
    "P_negative = num_negative/num_total\n",
    "P_neutral = num_neutral/num_total\n",
    "\n",
    "def naive_bayes(text):\n",
    "    tokens = preprocess_text(text, remove_stopwords=True)\n",
    "\n",
    "    # Initialize prob with prior prob\n",
    "    log_prob_positive = math.log(P_positive)\n",
    "    log_prob_negative = math.log(P_negative)\n",
    "    log_prob_neutral = math.log(P_neutral)\n",
    "\n",
    "    # Add log conditional prob to each word \n",
    "    for word in tokens:\n",
    "        if word in vocab:\n",
    "            log_prob_positive += math.log(conditional_probability_smoothing(word, 'positive'))\n",
    "            log_prob_negative += math.log(conditional_probability_smoothing(word, 'negative'))\n",
    "            log_prob_neutral += math.log(conditional_probability_smoothing(word, 'neutral'))\n",
    "    \n",
    "    if log_prob_positive > log_prob_negative and log_prob_positive > log_prob_neutral:\n",
    "        return 'positive' \n",
    "    elif log_prob_negative > log_prob_positive and log_prob_negative > log_prob_neutral:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# test messages\n",
    "test_messages = [\n",
    "    \"I just went to this Paris Baguette bakery coffee place in Uptown Phoenix and I can see why Starbucks is having trouble bouncing back. So many other bakery coffee places are popping up in so many places and are way better than Starbucks.\",  # likely negative\n",
    "    \"Omg I just tried the new matcha latte at starbucks and it's SO good!\",  # likely positive\n",
    "    \"been at starbucks for 40 minutes and dont even have a page of this damn paper done this is bad\" # likely neutral\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    result = naive_bayes(msg)\n",
    "    print(f'Message: {msg}')\n",
    "    print(f'Predicted Sentiment: {result}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406ecfa",
   "metadata": {},
   "source": [
    "#### 6. TEST OUR MODEL USING TRAIN_DF\n",
    "\n",
    "Now that we can predict whether a message is positive, negative, or neutral, we can individual run through each message in train_df and compare our new array of labels to the actual labels. We will test a few messages from train_df to show how our model is woking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c513e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: My cock is so hard while I sit at this Starbucks and talk to my sluts.\n",
      "neutral -> neutral\n",
      "\n",
      "\n",
      "message: Iâ€™ve worked at McDonalds and been a barista at Starbucks. I know the deal.\n",
      "neutral -> neutral\n",
      "\n",
      "\n",
      "message: iâ€™ve got a whole collection of my name misspelled on starbucks cups\n",
      "\n",
      "so now Iâ€™m curious, how do you think youâ€™re supposed to pronounce â€œaixâ€?\n",
      "neutral -> neutral\n",
      "\n",
      "\n",
      "message: Decided to try that new coffee shop downtown, will their latte be better than starbucks?\n",
      "neutral -> negative\n",
      "\n",
      "\n",
      "message: Omg I just reconnected with my old Starbucks barista so blessed\n",
      "positive -> neutral\n",
      "\n",
      "\n",
      "message: Smashing burgers to be a thick a Starbucks straw is unAmerican.\n",
      "neutral -> neutral\n",
      "\n",
      "\n",
      "message: I want to make a Starbucks pink drink.\n",
      "neutral -> positive\n",
      "\n",
      "\n",
      "message: Starbucks for the first time ðŸ¤£\n",
      "neutral -> positive\n",
      "\n",
      "\n",
      "message: In a Starbucks, striped sweater on, modest mouse playing, screenplay OUT WHO WANNA READ IT\n",
      "positive -> neutral\n",
      "\n",
      "\n",
      "message: Coffee is so much better when it's cold I'm really looking forward to that Iced coffee from Starbucks today\n",
      "positive -> positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = list(test_df['label'])\n",
    "y_pred = []\n",
    "\n",
    "for text in test_df['text']:\n",
    "    pred_label = naive_bayes(text)\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "test_df[\"predicted\"] = y_pred \n",
    "\n",
    "for i in range(10):\n",
    "    actual = test_df['label'][i]\n",
    "    predicted = test_df['predicted'][i]\n",
    "    print(f'message: {test_df['text'][i]}')\n",
    "    print(f'{actual} -> {predicted}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e43329d",
   "metadata": {},
   "source": [
    "#### 7. Analyze our Data \n",
    "\n",
    "To make it easier for us, we transferred our work here into 'naive_bayes.py' to run naive bayes on all of our csv files. We will use scikit's library to get the our classification report. This will tell us how our model performed in terms of accuracy, precision, recall, and f1 scoring. Our files include: \n",
    "\n",
    "- csv file 'starbucks.csv'\n",
    "- csv file 'starbucks2.csv'\n",
    "- last df holding both csv files \n",
    "- csv file 'dunkin.csv'\n",
    "- final df of all 3 files to maximize training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca37c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6564885496183206\n",
      "Precision: 0.6603256655190082\n",
      "Recall: 0.6564885496183206\n",
      "F1 Score: 0.650448632897622\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.39      0.48        28\n",
      "     neutral       0.74      0.73      0.73        62\n",
      "    positive       0.58      0.73      0.65        41\n",
      "\n",
      "    accuracy                           0.66       131\n",
      "   macro avg       0.64      0.62      0.62       131\n",
      "weighted avg       0.66      0.66      0.65       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('starbucks.csv')\n",
    "df1.drop_duplicates(subset=['text'], keep='last', inplace=True)\n",
    "df1.dropna()\n",
    "df1.reindex()\n",
    "\n",
    "starbucks1 = nb.naive_bayes(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0e79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6291390728476821\n",
      "Precision: 0.6246215704824977\n",
      "Recall: 0.6291390728476821\n",
      "F1 Score: 0.6196794476762884\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.62      0.61        53\n",
      "     neutral       0.60      0.39      0.48        38\n",
      "    positive       0.67      0.78      0.72        60\n",
      "\n",
      "    accuracy                           0.63       151\n",
      "   macro avg       0.62      0.60      0.60       151\n",
      "weighted avg       0.62      0.63      0.62       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('starbucks2.csv')\n",
    "df2.drop_duplicates(subset=['text'], keep='last', inplace=True)\n",
    "df2.dropna()\n",
    "df2.reindex()\n",
    "\n",
    "starbucks2 = nb.naive_bayes(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fb057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5567375886524822\n",
      "Precision: 0.5642241962126303\n",
      "Recall: 0.5567375886524822\n",
      "F1 Score: 0.5413435346282688\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.53      0.58        81\n",
      "     neutral       0.54      0.34      0.42       100\n",
      "    positive       0.53      0.79      0.63       101\n",
      "\n",
      "    accuracy                           0.56       282\n",
      "   macro avg       0.57      0.55      0.54       282\n",
      "weighted avg       0.56      0.56      0.54       282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sb = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "starbucks = nb.naive_bayes(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6ca64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 0.5489046344837629\n",
      "Recall: 0.5\n",
      "F1 Score: 0.4827500850892663\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.18      0.22        33\n",
      "     neutral       0.78      0.40      0.53        63\n",
      "    positive       0.46      0.76      0.57        66\n",
      "\n",
      "    accuracy                           0.50       162\n",
      "   macro avg       0.51      0.45      0.44       162\n",
      "weighted avg       0.55      0.50      0.48       162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('dunkin.csv')\n",
    "d.drop_duplicates(subset=['text'], keep='last', inplace=True)\n",
    "d.dropna(subset=['text','label'], inplace=True)\n",
    "d.reindex()\n",
    "\n",
    "dunkin = nb.naive_bayes(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb05878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54627539503386\n",
      "Precision: 0.5667467087885334\n",
      "Recall: 0.54627539503386\n",
      "F1 Score: 0.5320721352980172\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.41      0.49       114\n",
      "     neutral       0.60      0.39      0.48       163\n",
      "    positive       0.51      0.79      0.62       166\n",
      "\n",
      "    accuracy                           0.55       443\n",
      "   macro avg       0.57      0.53      0.53       443\n",
      "weighted avg       0.57      0.55      0.53       443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.concat([df1, df2, d], ignore_index=True)\n",
    "\n",
    "_NB = nb.naive_bayes(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0d79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "                                                text     label\n",
      "0  broccoli head just came in and had me make a l...   neutral\n",
      "1  I took my dog with me to Starbucks &amp; to go...   neutral\n",
      "2  Tomorrow Is Teacher Appreciation Week So All M...  positive\n",
      "3  why did starbucks just call us out infront of ...   neutral\n",
      "4  My cousins jst posted a story of them on a Sta...   neutral \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.29      0.40        28\n",
      "     neutral       0.60      0.90      0.72        62\n",
      "    positive       0.81      0.51      0.63        41\n",
      "\n",
      "    accuracy                           0.65       131\n",
      "   macro avg       0.69      0.57      0.58       131\n",
      "weighted avg       0.68      0.65      0.62       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load the data\n",
    "df = df\n",
    "\n",
    "# 2. Inspect the first few rows to confirm structure\n",
    "print(\"Sample data:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# 3. Split into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_df=0.9,\n",
    "    min_df=5\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 6. Train a Multinomial Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# 7. Evaluate on the test set\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
